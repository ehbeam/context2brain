{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(42)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utilities import load_coordinates, load_dtm, load_mini_batches\n",
    "from utilities import report_macro, report_class, compute_roc, compute_prc, plot_curves\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2\n",
    "batch_size = 64\n",
    "lr = 0.01\n",
    "weight_decay = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"lstm\" # Prefix for plot file names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "## Brain activation coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents   18155\n",
      "Structures  114\n"
     ]
    }
   ],
   "source": [
    "act_bin = load_coordinates()\n",
    "n_structs = act_bin.shape[1]\n",
    "print(\"{:12s}{}\".format(\"Documents\", act_bin.shape[0]))\n",
    "print(\"{:12s}{}\".format(\"Structures\", n_structs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Dimension  100\n",
      "Terms                350543\n"
     ]
    }
   ],
   "source": [
    "vsm = pd.read_csv(\"data/text/glove_gen_n100_win15_min5_iter500_190428.txt\", \n",
    "                  sep = \" \", index_col=0, header=0)\n",
    "n_emb = vsm.shape[1]\n",
    "print(\"{:21s}{}\".format(\"Embedding Dimension\", n_emb))\n",
    "print(\"{:21s}{}\".format(\"Terms\", vsm.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents   18155\n",
      "Terms       1542\n"
     ]
    }
   ],
   "source": [
    "dtm_bin = load_dtm()\n",
    "dtm_bin = dtm_bin[dtm_bin.columns.intersection(vsm.index)]\n",
    "n_terms = dtm_bin.shape[1]\n",
    "print(\"{:12s}{}\".format(\"Documents\", dtm_bin.shape[0]))\n",
    "print(\"{:12s}{}\".format(\"Terms\", n_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text features \n",
    "\n",
    "#### 1. Concatenate embeddings for terms in the lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 154200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = vsm.loc[dtm_bin.columns].values.reshape(1, n_terms*n_emb)\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create a mask for embeddings of terms that occurred in documents\n",
    "\n",
    "The mask is \"1\" for terms that occurred and \"0\" for terms that did not occur, with n_emb entries per term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18155, 154200)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_mask = np.repeat(dtm_bin.values, n_emb, axis=1)\n",
    "dtm_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Apply the mask to term embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18155, 154200)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_emb = dtm_mask * emb\n",
    "dtm_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_emb = pd.DataFrame(dtm_emb, index=dtm_bin.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data\n",
    "\n",
    "## Training and dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {}\n",
    "for split in [\"train\", \"dev\"]:\n",
    "    splits[split] = [int(pmid.strip()) for pmid in open(\"data/splits/{}.txt\".format(split), \"r\").readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dtm_emb.loc[splits[\"train\"]].transpose().values\n",
    "Y = act_bin.loc[splits[\"train\"]].transpose().values\n",
    "mini_batches = load_mini_batches(X, Y, mini_batch_size=batch_size, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 3\n",
    "lstm = nn.LSTM(n_emb*n_terms, n_emb*n_terms)  # Input is embedding dimension x N terms, output is N brain structures\n",
    "inputs = [torch.randn(1, n_emb*n_terms) for _ in range(context_length)]  # Make a sequence of length context_length\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, n_emb*n_terms), torch.randn(1, 1, n_emb*n_terms))  # Clean out hidden state\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_terms, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, 100)\n",
    "        self.fc4 = nn.Linear(100, 100)\n",
    "        self.fc5 = nn.Linear(100, 100)\n",
    "        self.fc6 = nn.Linear(100, 100)\n",
    "        self.fc7 = nn.Linear(100, 100)\n",
    "        self.fc8 = nn.Linear(100, 100)\n",
    "        self.fc9 = nn.Linear(100, 100)\n",
    "        self.fc10 = nn.Linear(100, 100)\n",
    "        self.fc11 = nn.Linear(100, n_structs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = F.relu(self.fc8(x))\n",
    "        x = F.relu(self.fc9(x))\n",
    "        x = F.relu(self.fc10(x))\n",
    "        x = torch.sigmoid(self.fc11(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = F.binary_cross_entropy\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):  # Loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(mini_batches):\n",
    "        \n",
    "        # Get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = Variable(torch.from_numpy(inputs.T).float())\n",
    "        labels = Variable(torch.from_numpy(labels.T).float())\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print running loss\n",
    "    running_loss += loss.item()\n",
    "    print(\"Epoch {:3d} \\t Loss {:6.6f}\".format(epoch + 1, running_loss / 100))\n",
    "    running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_metrics(data_set):\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = data_set[0]\n",
    "        inputs = Variable(torch.from_numpy(inputs.T).float())\n",
    "        labels = Variable(torch.from_numpy(labels.T).float())\n",
    "        outputs = net(inputs)\n",
    "        predictions = (outputs > 0.5).float() * 1\n",
    "        print(\"-\" * 50 + \"\\nMACRO-AVERAGED TOTAL\\n\" + \"-\" * 50)\n",
    "        report_macro(labels, predictions)\n",
    "        print(\"\\n\" + \"-\" * 50 + \"\\n\\n\")\n",
    "        for i in range(n_structs):\n",
    "            print(\"-\" * 50 + \"\\n\" + act_bin.columns[i].title().replace(\"_\", \" \") + \"\\n\" + \"-\" * 50)\n",
    "            report_class(labels[:,i], predictions[:,i])\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_curves(data_set, name): \n",
    "    with torch.no_grad():\n",
    "        inputs, labels = data_set[0]\n",
    "        inputs = Variable(torch.from_numpy(inputs.T).float())\n",
    "        labels = Variable(torch.from_numpy(labels.T).float())\n",
    "        pred_probs = net(inputs).float()\n",
    "        fpr, tpr = compute_roc(labels, pred_probs)\n",
    "        prec, rec = compute_prc(labels, pred_probs)\n",
    "        plot_curves(\"{}_roc\".format(name), fpr, tpr, diag=True, alpha=0.25,\n",
    "                    xlab=\"1 - Specificity\", ylab=\"Sensitivity\")\n",
    "        plot_curves(\"{}_prc\".format(name), rec, prec, diag=False, alpha=0.5,\n",
    "                    xlab=\"Recall\", ylab=\"Precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = dtm_bin.loc[splits[\"train\"]].transpose().values\n",
    "Y_train = act_bin.loc[splits[\"train\"]].transpose().values\n",
    "train_set = load_mini_batches(X_train, Y_train, mini_batch_size=len(splits[\"train\"]), seed=42)\n",
    "report_curves(train_set, \"{}_train\".format(prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_metrics(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev set performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_dev = dtm_bin.loc[splits[\"dev\"]].transpose().values\n",
    "Y_dev = act_bin.loc[splits[\"dev\"]].transpose().values\n",
    "dev_set = load_mini_batches(X_dev, Y_dev, mini_batch_size=len(splits[\"dev\"]), seed=42)\n",
    "report_curves(dev_set, \"{}_dev\".format(prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_metrics(dev_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Net.state_dict(), \"models/{}.pt\".format(prefix))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Ontol)",
   "language": "python",
   "name": "ontol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
